I'm Michael Hanna, a PhD student at the [ILLC](https://www.illc.uva.nl/) at the University of Amsterdam, supervised by [Sandro Pezzelle](https://sandropezzelle.github.io/) and [Yonatan Belinkov](https://www.cs.technion.ac.il/~belinkov/), through [ELLIS](https://ellis.eu/projects/interpreting-nlp-models-through-the-lens-of-cognition-and-linguistics). I'm interested in interpreting and evaluating NLP models by combining techniques from diverse fields such as cognitive science and mechanistic interpretability.

**Are you an AI master's student at the UvA looking to write a thesis on mechanistic interpretability?**  I plan to supervise one student this year. I'm currently interested in [(transcoder) feature circuits](https://transformer-circuits.pub/2025/attribution-graphs/methods.html), but am open to other project ideas as well! Send me an email with your CV and interpretability experience, including any relevant projects you've done. Feel free to send over some thesis project ideas as well, if there's anything in particular you'd like to work on!

## News
- **August 2025**: New [paper](https://arxiv.org/abs/2503.11302) on understanding the formal-functional divide using component circuits will appear in [Computational Linguistics](https://submissions.cljournal.org/index.php/cljournal/index)!
- **May 2025**:
  - My Anthropic Fellows partner Mateusz Piotrowski and I have released [circuit-tracer](https://github.com/safety-research/circuit-tracer), a library for transcoder circuit finding in open-source LLMs! Try it out on [Neuronpedia](https://www.neuronpedia.org/gemma-2-2b/graph?slug=gemma-fact-dallas-austin&pinnedIds=27_22605_10%2C20_15589_10%2CE_26865_9%2C21_5943_10%2C23_12237_10%2C20_15589_9%2C16_25_9%2C14_2268_9%2C18_8959_10%2C4_13154_9%2C7_6861_9%2C19_1445_10%2CE_2329_7%2CE_6037_4%2C0_13727_7%2C6_4012_7%2C17_7178_10%2C15_4494_4%2C6_4662_4%2C4_7671_4%2C3_13984_4%2C1_1000_4%2C19_7477_9%2C18_6101_10%2C16_4298_10%2C7_691_10&supernodes=%5B%5B%22state%22%2C%226_4012_7%22%2C%220_13727_7%22%5D%2C%5B%22preposition+followed+by+place+name%22%2C%2219_1445_10%22%2C%2218_6101_10%22%5D%2C%5B%22Texas%22%2C%2220_15589_10%22%2C%2220_15589_9%22%2C%2219_7477_9%22%2C%2216_25_9%22%2C%224_13154_9%22%2C%2214_2268_9%22%2C%227_6861_9%22%5D%2C%5B%22capital+%2F+capital+cities%22%2C%2215_4494_4%22%2C%226_4662_4%22%2C%224_7671_4%22%2C%223_13984_4%22%2C%221_1000_4%22%2C%2221_5943_10%22%2C%2217_7178_10%22%2C%227_691_10%22%2C%2216_4298_10%22%5D%5D&pruningThreshold=0.6&clickedId=21_5943_10&densityThreshold=0.99) - it only takes a few seconds. Thanks a bunch to Emmanuel Ameisen and Jack Lindsey for supervising, as well as Johnny Lin for making the Neuronpedia integration go so smoothly.
  - Many colleagues and I have a [paper](https://arxiv.org/abs/2504.13151) accepted to ICML 2025 on a new Mechanistic Interpretability Benchmark, for both causal graph and circuit identification!
  - Another big group [paper](https://arxiv.org/abs/2406.18403) on LLM judges has been accepted to ACL 2025!
- **March 2025**: I'm in Berkeley until September as an [Anthropic Research Fellow](https://alignment.anthropic.com/2024/anthropic-fellows-program/)!
- **January 2025**: New [paper](https://aclanthology.org/2025.naacl-long.164/) accepted to NAACL 2025, coauthored with [Aaron Mueller](https://aaronmueller.github.io/) on using sparse autoencoders to understand how language models incrementally process sentences!
- **December 2024**: My colleagues and I have released short videos explaining basic concepts in interpretability! My advisor, Sandro Pezzelle, and I talk about circuits; check out the video [here](https://projects.illc.uva.nl/indeep/indeep-video-series/)!
- **September 2024**: New [paper](https://openreview.net/forum?id=3Ds5vNudIE), led by [Curt Tigges](https://curttigges.com/), accepted to NeurIPS 2024, and appeared as an oral at [RepL4NLP2024](https://sites.google.com/view/repl4nlp2024/) at ACL!
- **July 2024**: New [paper](https://openreview.net/forum?id=TZ0CCGDcuT) on faithfulness in circuit-finding will appear at [COLM](https://colmweb.org/) and as a spotlight at the [ICML 2024 Mech. Interp. Workshop](https://icml2024mi.pages.dev/) ([poster](https://hannamw.github.io/preprints-posters/mech_interp_workshop_poster.pdf))!
- **May 2024**: New [paper](https://aclanthology.org/2024.findings-acl.572/) on underspecification + LMs accepted to the Findings of ACL 2024; congrats to Frank Wildenburg, the master's student who led this project!
- **April 2024**: I have received a [Superalignment Grant](https://openai.com/blog/superalignment-fast-grants) from OpenAI! I'm very excited to work on improving our mechanistic understanding of how larger models' capabilities differ from those of smaller models.
- **March 2023**: Along with [Jaap Jumelet](https://jumelet.ai/), [Hosein Mohebbi](https://hmohebbi.github.io/), [Afra Alishahi](https://afra.alishahi.name/), and [Jelle Zuidema](https://staff.fnwi.uva.nl/w.zuidema/), I gave a [tutorial on Transformer-specific Interpretability](https://projects.illc.uva.nl/indeep/tutorial/) at [EACL 2024](https://2024.eacl.org/), on March 24th! My session focused on circuits in pre-trained transformer language models; find the recording [here](https://youtu.be/qno_Y_qpTrc).
- **November 2023**: I've been awarded a prize from [CIMeC](https://www.cimec.unitn.it/en) and the [Fondazione Alvise Comel](https://agiati.org/fondazione-alvise-comel) for being one of the top two master's theses that connect AI and cognitive neuroscience!
- **October 2023**: New [paper](https://aclanthology.org/2023.emnlp-main.744/) on transformer LMs' animacy processing accepted to EMNLP 2023!
- **September 2023**: New [paper](https://openreview.net/forum?id=p4PckNQR8k) on GPT-2's partially generalizable circuit for greater-than has been accepted to NeurIPS 2023!
- **January 2023**:
  - New [paper](https://aclanthology.org/2023.eacl-main.58/) based on my master's thesis accepted to EACL!
  - I spent January 2023 participating in Redwood Research's [REMIX](https://www.redwoodresearch.org/remix) program!
- **September 2022**: I graduated from the University of Trento (Cognitive Science) and Charles University in Prague (Computer Science), as part of the [EM LCT](https://lct-master.org/) dual-degree master's program! My thesis, supervised by [Roberto Zamparelli](https://webapps.unitn.it/du/en/Persona/PER0001015/Curriculum) and [David Mareček](https://ufal.mff.cuni.cz/david-marecek), is available [here](https://hannamw.github.io/preprints-posters/thesis_michael_hanna.pdf).
- **August 2022**: New [paper](https://aclanthology.org/2022.coling-1.495/) at [COLING 2022](https://coling2022.org/)!
- **November 2021**: New papers at [WMT 2021](https://aclanthology.org/2021.wmt-1.59/) and [BlackboxNLP 2021](https://aclanthology.org/2021.blackboxnlp-1.20/)!
- **June 2020**: I graduated from the University of Chicago (CS and Linguistics)! I wrote an honors thesis on interpreting variational autoencoders for sentences, advised by [Allyson Ettinger](https://aetting.github.io/).

## Selected Publications 
For a full list, see [Publications](https://hannamw.github.io/publications/) or my [Google Scholar](https://scholar.google.com/citations?user=0wOdTeYAAAAJ&hl=en).

**Michael Hanna**, Yonatan Belinkov, and Sandro Pezzelle. 2025. [Are Formal and Functional Linguistic Mechanisms Dissociated in Language Models?](https://arxiv.org/abs/2503.11302). To appear in Computational Linguistics. **(Computational Linguistics)**

**Michael Hanna*** and Aaron Mueller*. 2025. [Incremental Sentence Processing Mechanisms in Autoregressive Transformer Language Models](https://aclanthology.org/2025.naacl-long.164/). In the 2025 Conference of the Nations of the Americas Chapter of the Association for Computational Linguistics. **(NAACL 2025)** 

Curt Tigges, **Michael Hanna**, Qinan Yu, Stella Biderman. 2024. [LLM Circuit Analyses Are Consistent Across Training and Scale](https://openreview.net/forum?id=3Ds5vNudIE). In the Thirty-eight Conference on Neural Information Processing Systems. **(NeurIPS 2024)**

**Michael Hanna**, Sandro Pezzelle, and Yonatan Belinkov. 2024. [Have Faith in Faithfulness: Going Beyond Circuit Overlap When Finding Model Mechanisms](https://openreview.net/forum?id=TZ0CCGDcuT). In the First Conference on Language Modeling (COLM). **(COLM 2024)**

**Michael Hanna**, Yonatan Belinkov, and Sandro Pezzelle. 2023. [When Language Models Fall in Love: Animacy Processing in Transformer Language Models](https://aclanthology.org/2023.emnlp-main.744/). In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing (EMNLP). **(EMNLP 2023)**

**Michael Hanna**, Ollie Liu, and Alexandre Variengien. 2023. [How does GPT-2 compute greater-than?: Interpreting mathematical abilities in a pre-trained language model](https://openreview.net/forum?id=p4PckNQR8k). In the Thirty-seventh Conference on Neural Information Processing Systems. **(NeurIPS 2023)**

**Michael Hanna**, Roberto Zamparelli, and David Mareček. 2023. [The Functional Relevance of Probed Information: A Case Study](https://aclanthology.org/2023.eacl-main.58/). In Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics, pages 835–848, Dubrovnik, Croatia. Association for Computational Linguistics. **(EACL 2023)**

**Michael Hanna** and David Marecek. 2021. [Investigating BERT’s Knowledge of Hypernymy through Prompting](https://aclanthology.org/2021.blackboxnlp-1.20/). In Proceedings of the Fourth BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP. Punta Cana, Dominican Republic. Association for Computational Linguistics. **(BlackBoxNLP 2021)**

## Academic Interests
My main research interest, interpretability in the context of modern language models, is twofold. First, I'm interested in asking "What are the abilities of these models?" from a perspective informed by linguistics and cognitive science. In this so-called behavioral paradigm, I study these models on a pure input-output level, leveraging the wide body of psycholinguistic research conducted on humans. Second, I'm interested in answering "How do these models achieve such impressive performance on linguistic tasks?", using techniques from (mechanistic) interpretability. I use causal interventions to perform low-level studies of language models, uncovering the mechanisms that drive their behavior.

## Personal Interests
- **Living Abroad**: I've been able to spend time living in a variety of foreign countries through various different programs, including Spain ([SYA](https://www.sya.org/)); Korea ([NSLI-Y](https://www.nsliforyouth.org/), [FLAG](https://study-abroad.uchicago.edu/summer-grant/foreign-language-acquisition-grant-flag)); Czechia and Italy ([LCT](https://lct-master.org/)); and the Netherlands and (soon) Israel ([ELLIS](https://ellis.eu/)). If you're in high school or university and want to know more about opportunities like this, shoot me an email—I'm happy to share more info about programs that will let you live abroad, oftentimes for free!
- **Urbanism**: I'm fascinated by the way that our built environment affects our lives. I'm a big fan of walkable neighborhoods, public transportation, and the power of well-designed spaces to bring people together.
- **Vocal Performance**: During my undergrad, I was a baritone in [Run for Cover](http://runforcover.uchicago.edu/), an a cappella group at UChicago. Check us out on [Spotify](https://play.spotify.com/artist/1WN22dBwn6fM3biZufox5W) or [iTunes](https://itunes.apple.com/us/artist/run-for-cover/id848631625)!
