I'm Michael Hanna, a PhD student at the [ILLC](https://www.illc.uva.nl/) at the University of Amsterdam, supervised by [Sandro Pezzelle](https://sandropezzelle.github.io/) and [Yonatan Belinkov](https://www.cs.technion.ac.il/~belinkov/), through [ELLIS](https://ellis.eu/projects/interpreting-nlp-models-through-the-lens-of-cognition-and-linguistics). I'm interested in interpreting and evaluating NLP models by combining techniques from diverse fields such as cognitive science and mechanistic interpretability.

## News
- **March 2023**: New [pre-print](https://arxiv.org/abs/2403.17806) on the importance of faithfulness in circuit-finding, and how to find more faithful circuits, automatically.
- **March 2023**: Along with [Jaap Jumelet](https://jumelet.ai/), [Hosein Mohebbi](https://hmohebbi.github.io/), [Afra Alishahi](https://afra.alishahi.name/), and [Jelle Zuidema](https://staff.fnwi.uva.nl/w.zuidema/), I gave a [tutorial on Transformer-specific Interpretability](https://projects.illc.uva.nl/indeep/tutorial/) at [EACL 2024](https://2024.eacl.org/), on March 24th! My session focused on circuits in pre-trained transformer language models; find the recording [here](https://youtu.be/qno_Y_qpTrc).
- **December 2023**: I was at EMNLP and NeurIPS 2023! Here are links to the work I presented at EMNLP ([paper](https://arxiv.org/abs/2310.15004), [poster](https://hannamw.github.io/papers/emnlp2023_animacy_poster.pdf)) and NeurIPS ([paper](http://arxiv.org/abs/2305.00586), [poster](https://hannamw.github.io/papers/NeurIPS2023_poster.pdf)).
- **November 2023**: I've been awarded a prize from [CIMeC](https://www.cimec.unitn.it/en) and the [Fondazione Alvise Comel](https://agiati.org/fondazione-alvise-comel) for being one of the top two master's theses that connect AI and cognitive neuroscience!
- **October 2023**: New [paper](https://arxiv.org/abs/2310.15004) on transformer LMs' animacy processing accepted to EMNLP 2023!
- **September 2023**: New [paper](http://arxiv.org/abs/2305.00586) on GPT-2's partially generalizable circuit for greater-than has been accepted to NeurIPS 2023!
- **July-September 2023**: I attended [Lisbon Machine Learning Summer School](http://lxmls.it.pt/2023/), and the [Analytical Connectionism Summer Course](https://www.ucl.ac.uk/gatsby/analytical-connectionism-2023)! I presented [this poster](https://hannamw.github.io/papers/ac2023_poster.pdf) at the latter school.
- **January 2023**: New [paper](https://aclanthology.org/2023.eacl-main.58/) based on my master's thesis accepted to EACL!
- **January 2023**: I spent January 2023 participating in Redwood Research's [REMIX](https://www.redwoodresearch.org/remix) program!
- **September 2022**: I graduated from the University of Trento (Cognitive Science) and Charles University in Prague (Computer Science), as part of the [EM LCT](https://lct-master.org/) dual-degree master's program! My thesis, supervised by [Roberto Zamparelli](https://webapps.unitn.it/du/en/Persona/PER0001015/Curriculum) and [David Mareček](https://ufal.mff.cuni.cz/david-marecek), is available [here](https://hannamw.github.io/papers/thesis_michael_hanna.pdf).
- **August 2022**: New [paper](https://aclanthology.org/2022.coling-1.495/) at [COLING 2022](https://coling2022.org/)!
- **November 2021**: New papers at [WMT 2021](https://aclanthology.org/2021.wmt-1.59/) and [BlackboxNLP 2021](https://aclanthology.org/2021.blackboxnlp-1.20/)!
- **June 2020**: I graduated from the University of Chicago (CS and Linguistics)! I wrote an honors thesis on interpreting variational autoencoders for sentences, advised by [Allyson Ettinger](https://aetting.github.io/).

## Selected Publications 
For a full list, see [Publications](https://hannamw.github.io/publications/). 

**Michael Hanna**, Yonatan Belinkov, and Sandro Pezzelle. 2023. [When Language Models Fall in Love: Animacy Processing in Transformer Language Models](https://arxiv.org/abs/2310.15004). In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing (EMNLP). **(EMNLP 2023)**

**Michael Hanna**, Ollie Liu, and Alexandre Variengien. 2023. [How does GPT-2 compute greater-than?: Interpreting mathematical abilities in a pre-trained language model](http://arxiv.org/abs/2305.00586). In the Thirty-seventh Conference on Neural Information Processing Systems. **(NeurIPS 2023)**

**Michael Hanna**, Roberto Zamparelli, and David Mareček. 2023. [The Functional Relevance of Probed Information: A Case Study](https://aclanthology.org/2023.eacl-main.58/). In Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics, pages 835–848, Dubrovnik, Croatia. Association for Computational Linguistics. **(EACL 2023)**

**Michael Hanna\***, Federico Pedeni\*, Alessandro Suglia, Alberto Testoni, and Raffaella Bernardi. 2022. [ACT-Thor: A Controlled Benchmark for Embodied Action Understanding in Simulated Environments](https://aclanthology.org/2022.coling-1.495/). In Proceedings of the 29th International Conference on Computational Linguistics, Gyeongju, Republic of Korea. International Committee on Computational Linguistics. **(COLING 2022)**

**Michael Hanna** and David Marecek. 2021. [Investigating BERT’s Knowledge of Hypernymy through Prompting](https://aclanthology.org/2021.blackboxnlp-1.20/). In Proceedings of the Fourth BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP. Punta Cana, Dominican Republic. Association for Computational Linguistics. **(BlackBoxNLP 2021)**

## Academic Interests
My main research interest, interpretability in the context of modern language models, is twofold. First, I'm interested in asking "What are the abilities of these models?" from a perspective informed by linguistics and cognitive science. In this so-called behavioral paradigm, I study these models on a pure input-output level, leveraging the wide body of psycholinguistic research conducted on humans. Second, I'm interested in answering "How do these models achieve such impressive performance on linguistic tasks?", using techniques from (mechanistic) interpretability. I use causal interventions to perform low-level studies of language models, uncovering the mechanisms that drive their behavior.

## Personal Interests
- **Living Abroad**: I've been able to spend time living in a variety of foreign countries through various different programs, including Spain ([SYA](https://www.sya.org/)); Korea ([NSLI-Y](https://www.nsliforyouth.org/), [FLAG](https://study-abroad.uchicago.edu/summer-grant/foreign-language-acquisition-grant-flag)); Czechia and Italy ([LCT](https://lct-master.org/)); and the Netherlands and (soon) Israel ([ELLIS](https://ellis.eu/)). If you're in high school or university and want to know more about opportunities like this, shoot me an email—I'm happy to share more info about programs that will let you live abroad, oftentimes for free!
- **Urbanism**: I'm fascinated by the way that our built environment affects our lives. I'm a big fan of walkable neighborhoods, public transportation, and the power of well-designed spaces to bring people together.
- **Vocal Performance**: During my undergrad, I was a baritone in [Run for Cover](http://runforcover.uchicago.edu/), an a cappella group at UChicago. Check us out on [Spotify](https://play.spotify.com/artist/1WN22dBwn6fM3biZufox5W) or [iTunes](https://itunes.apple.com/us/artist/run-for-cover/id848631625)!
