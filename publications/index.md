# Publications
## Conference Publications
**Michael Hanna*** and Aaron Mueller*. 2025. [Incremental Sentence Processing Mechanisms in Autoregressive Transformer Language Models](https://arxiv.org/abs/2412.05353). To appear at the 2025 Conference of the Nations of the Americas Chapter of the Association for Computational Linguistics. **(NAACL 2025)** 

Curt Tigges, **Michael Hanna**, Qinan Yu, Stella Biderman. 2024. [LLM Circuit Analyses Are Consistent Across Training and Scale](https://arxiv.org/abs/2407.10827). In the Thirty-eight Conference on Neural Information Processing Systems. **(NeurIPS 2024)**

**Michael Hanna**, Sandro Pezzelle, and Yonatan Belinkov. 2024. [Have Faith in Faithfulness: Going Beyond Circuit Overlap When Finding Model Mechanisms](https://arxiv.org/abs/2403.17806). In the First Conference on Language Modeling. **(COLM 2024)**

Frank Wildenburg, **Michael Hanna**, and Sandro Pezzelle. 2024. [Do Pre-Trained Language Models Detect and Understand Semantic Underspecification? Ask the DUST!](https://arxiv.org/abs/2402.12486). In Findings of the Association for Computational Linguistics **(ACL Findings 2024)**

**Michael Hanna**, Yonatan Belinkov, and Sandro Pezzelle. 2023. [When Language Models Fall in Love: Animacy Processing in Transformer Language Models](https://aclanthology.org/2023.emnlp-main.744/). In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing (EMNLP). **(EMNLP 2023)**

**Michael Hanna**, Ollie Liu, and Alexandre Variengien. 2023. [How does GPT-2 compute greater-than?: Interpreting mathematical abilities in a pre-trained language model](http://arxiv.org/abs/2305.00586). In the Thirty-seventh Conference on Neural Information Processing Systems. **(NeurIPS 2023)**

**Michael Hanna**, Roberto Zamparelli, and David Mareček. 2023. [The Functional Relevance of Probed Information: A Case Study](https://aclanthology.org/2023.eacl-main.58/). In Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics, Dubrovnik, Croatia. Association for Computational Linguistics. **(EACL 2023)**

**Michael Hanna\***, Federico Pedeni\*, Alessandro Suglia, Alberto Testoni, and Raffaella Bernardi. 2022. [ACT-Thor: A Controlled Benchmark for Embodied Action Understanding in Simulated Environments](https://aclanthology.org/2022.coling-1.495/). In Proceedings of the 29th International Conference on Computational Linguistics, Gyeongju, Republic of Korea. International Committee on Computational Linguistics. **(COLING 2022)**

## (Archival) Workshop Publications
Abhijith Chintam, Rahel Beloch, Willem Zuidema, **Michael Hanna\***, and Oskar van der Wal\*. 2023. [Identifying and Adapting Transformer-Components Responsible for Gender Bias in an English Language Model](https://aclanthology.org/2023.blackboxnlp-1.29/). In Proceedings of the Sixth BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP. **(BlackBoxNLP 2023)**

Jaap Jumelet, **Michael Hanna\***, Marianne de Heer Kloots\*, Anne Langedijk\*, Charlotte Pouw\*, and Oskar van der Wal\*. 2023. [ChapGTP, ILLC’s Attempt at Raising a BabyLM: Improving Data Efficiency by Automatic Task Formation](https://aclanthology.org/2023.conll-babylm.6/). **(BabyLM Challenge 2023)**

**Michael Hanna** and Ondrej Bojar. 2021. [A Fine-Grained Investigation of BERTScore](https://aclanthology.org/2021.wmt-1.59/). In Proceedings of the Sixth Conference on Machine Translation. Punta Cana, Dominican Republic (Online). Association for Computational Linguistics. **(WMT 2021)**

**Michael Hanna** and David Marecek. 2021. [Investigating BERT’s Knowledge of Hypernymy through Prompting](https://aclanthology.org/2021.blackboxnlp-1.20/). In Proceedings of the Fourth BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP. Punta Cana, Dominican Republic. Association for Computational Linguistics. **(BlackBoxNLP 2021)**

\* Equal contribution

<!---
## Software Engineering Projects

### Learn Hangul (2019)
<img src="/assets/img/Thumb.png" width="140" height="140">

A learning game (think Duolingo) that teaches you Hangul / 한글, the Korean alphabet. The game is written entirely in Elm, a purely functional language designed for reliable webapps with no runtime exceptions—a property of Elm that I hope you enjoy while playing this game!

This was developed as a project for CMSC 22300: Functional Programming. It does not yet teach all elements of Hangul.

Play the demo [here](https://hannamw.github.io/demos/learn-hangul)!

### Othello (2017)
<img src="/assets/img/othello.png" width="140" height="140">

An implementation of the classic board game, Othello, in C. Play against a human, or a computer. The computer can be very hard to beat - at its hardest setting, it uses a minimax strategy that evaluates board states up to 6 moves in advance.

This program can found on <a href="https://github.com/hannamw/othello-in-c">github</a>, and is run from the command line.
-->
